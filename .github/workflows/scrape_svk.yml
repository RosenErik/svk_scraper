name: Scrape SVK Power Data

on:
  schedule:
    # Run at 06:00 UTC (08:00 Swedish time) every day
    - cron: '0 6 * * *'
  workflow_dispatch:  # Allow manual trigger
    inputs:
      days_to_scrape:
        description: 'Number of days to scrape (default: 3)'
        required: false
        default: '3'
      start_date:
        description: 'Start date (YYYY-MM-DD) - optional'
        required: false
        default: ''

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
    - name: Install ChromeDriver
      run: |
        # Get Chrome version
        CHROME_VERSION=$(google-chrome --version | awk '{print $3}')
        echo "Chrome version: $CHROME_VERSION"
        
        # Get the major version number
        CHROME_MAJOR=$(echo $CHROME_VERSION | awk -F'.' '{print $1}')
        echo "Chrome major version: $CHROME_MAJOR"
        
        # Get the latest ChromeDriver version for this Chrome version
        # Using the Chrome for Testing availability API
        DRIVER_VERSION=$(curl -s "https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_${CHROME_MAJOR}")
        echo "ChromeDriver version: $DRIVER_VERSION"
        
        # Download ChromeDriver
        wget -q -O /tmp/chromedriver-linux64.zip "https://storage.googleapis.com/chrome-for-testing-public/${DRIVER_VERSION}/linux64/chromedriver-linux64.zip"
        
        # Extract and install
        unzip -q /tmp/chromedriver-linux64.zip -d /tmp/
        sudo mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver
        
        # Verify installation
        chromedriver --version
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install selenium pandas openpyxl
        
    - name: Run scraper
      run: |
        # Set default values
        DAYS="${{ github.event.inputs.days_to_scrape }}"
        DAYS="${DAYS:-3}"  # Default to 3 days if not specified
        
        START_DATE="${{ github.event.inputs.start_date }}"
        
        echo "Scraping $DAYS days starting from ${START_DATE:-'today'}"
        
        # Run the main scraper script
        python scripts/run_scraper.py --days "$DAYS" ${START_DATE:+--start-date "$START_DATE"}
        
    - name: Commit and push data
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Check if there are changes
        if [[ -n $(git status -s data/) ]]; then
          git add data/
          git commit -m "Update SVK power data - $(date +'%Y-%m-%d %H:%M:%S UTC')"
          git push
          echo "✅ New data committed and pushed"
        else
          echo "ℹ️ No new data to commit"
        fi
        
    - name: Upload artifacts (for debugging)
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: error-screenshots
        path: |
          *.png
          *.log